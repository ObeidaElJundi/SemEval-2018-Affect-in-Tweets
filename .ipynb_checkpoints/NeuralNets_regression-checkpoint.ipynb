{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from resources.arabic_preprocessing import Arabic_preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-ar-joy-train-1</td>\n",
       "      <td>منسم يقول لصيدلي\\n\\nعندك شريط بنادول؟\\nقاله:نع...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-ar-joy-train-2</td>\n",
       "      <td>· بتكبرين ياللي من رضا الأيام جيتي يوم ميلادك ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-ar-joy-train-3</td>\n",
       "      <td>#هدوء #عمرو دياب #مزاج ❤️❤️❤️❤️❤️</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-ar-joy-train-4</td>\n",
       "      <td>سبحان الله #رضى الوالدين له سحر عجيب</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-ar-joy-train-5</td>\n",
       "      <td>الله يرضى عليك. \\nبدل متقولي: (عيد سعيد)، قولي...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 index                                              tweet  \\\n",
       "0  2018-ar-joy-train-1  منسم يقول لصيدلي\\n\\nعندك شريط بنادول؟\\nقاله:نع...   \n",
       "1  2018-ar-joy-train-2  · بتكبرين ياللي من رضا الأيام جيتي يوم ميلادك ...   \n",
       "2  2018-ar-joy-train-3                  #هدوء #عمرو دياب #مزاج ❤️❤️❤️❤️❤️   \n",
       "3  2018-ar-joy-train-4               سبحان الله #رضى الوالدين له سحر عجيب   \n",
       "4  2018-ar-joy-train-5  الله يرضى عليك. \\nبدل متقولي: (عيد سعيد)، قولي...   \n",
       "\n",
       "  emotion  score  \n",
       "0     joy  0.422  \n",
       "1     joy  0.565  \n",
       "2     joy  0.438  \n",
       "3     joy  0.375  \n",
       "4     joy  0.438  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample=pd.read_csv('data/joy_train.csv',header=None, names=['index', 'tweet','emotion','score'])\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-ar-joy-train-1</td>\n",
       "      <td>منسم قول لصيدلي عند شريط بنادول قالهنعم قال شغ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-ar-joy-train-2</td>\n",
       "      <td>· بتكبر الي رضا ايم جيتي ميلاد فرح عمر مو اريخ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-ar-joy-train-3</td>\n",
       "      <td>هدوء عمرو دياب مزاج حب ️ حب ️ حب ️ حب ️ حب ️</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-ar-joy-train-4</td>\n",
       "      <td>اله رضي والد سحر عجيب</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-ar-joy-train-5</td>\n",
       "      <td>اله رضي بدل متقولي عيد سعيد قولي قدام 6 ليالي ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 index                                              tweet  \\\n",
       "0  2018-ar-joy-train-1  منسم قول لصيدلي عند شريط بنادول قالهنعم قال شغ...   \n",
       "1  2018-ar-joy-train-2  · بتكبر الي رضا ايم جيتي ميلاد فرح عمر مو اريخ...   \n",
       "2  2018-ar-joy-train-3       هدوء عمرو دياب مزاج حب ️ حب ️ حب ️ حب ️ حب ️   \n",
       "3  2018-ar-joy-train-4                              اله رضي والد سحر عجيب   \n",
       "4  2018-ar-joy-train-5  اله رضي بدل متقولي عيد سعيد قولي قدام 6 ليالي ...   \n",
       "\n",
       "  emotion  score  \n",
       "0     joy  0.422  \n",
       "1     joy  0.565  \n",
       "2     joy  0.438  \n",
       "3     joy  0.375  \n",
       "4     joy  0.438  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep = Arabic_preprocessing()\n",
    "sample['tweet']=sample['tweet'].apply(lambda x : prep.preprocess_arabic_text(x))\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3019\n"
     ]
    }
   ],
   "source": [
    "X = sample['tweet'].tolist()\n",
    "y_train = sample['score'].tolist()\n",
    "count_vect = CountVectorizer(lowercase=False, token_pattern=r'\\S+')\n",
    "count_vect = count_vect.fit(X)  # bag of words\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf = tfidf.fit(count_vect.transform(X))\n",
    "X_train_count = count_vect.transform(X)\n",
    "X_train_tfidf = tfidf.transform(X_train_count.toarray())\n",
    "input_count = X_train_count.shape[1]\n",
    "print(input_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the model : Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_model(input_len, learning_rate=0.001):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4000, input_dim=input_len, activation='relu'))\n",
    "    model.add(Dense(2000, activation='relu'))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(1000, activation='relu'))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    # Compile model\n",
    "    rmsprop = RMSprop(lr=learning_rate)\n",
    "    model.compile(loss='mse', optimizer=rmsprop, metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "**run either 'Without GridSearch' or 'Manual Grid Search', not both!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "600/600 [==============================] - 10s - loss: 2.8037 - mean_squared_error: 2.8037    \n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 9s - loss: 0.0345 - mean_squared_error: 0.0345     \n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 9s - loss: 0.0222 - mean_squared_error: 0.0222     \n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 9s - loss: 0.0223 - mean_squared_error: 0.0223     \n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 9s - loss: 0.0205 - mean_squared_error: 0.0205     \n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 9s - loss: 0.0157 - mean_squared_error: 0.0157     \n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 9s - loss: 0.0143 - mean_squared_error: 0.0143     \n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 9s - loss: 0.0108 - mean_squared_error: 0.0108     \n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 9s - loss: 0.0088 - mean_squared_error: 0.0088     \n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 9s - loss: 0.0091 - mean_squared_error: 0.0091     \n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 9s - loss: 0.0070 - mean_squared_error: 0.0070     \n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 9s - loss: 0.0057 - mean_squared_error: 0.0057     \n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 9s - loss: 0.0046 - mean_squared_error: 0.0046     \n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 9s - loss: 0.0053 - mean_squared_error: 0.0053     \n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 9s - loss: 0.0036 - mean_squared_error: 0.0036     \n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 9s - loss: 0.0041 - mean_squared_error: 0.0041     \n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 9s - loss: 0.0031 - mean_squared_error: 0.0031     \n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 9s - loss: 0.0026 - mean_squared_error: 0.0026     \n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 9s - loss: 0.0031 - mean_squared_error: 0.0031     \n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 9s - loss: 0.0023 - mean_squared_error: 0.0023     \n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 9s - loss: 0.0021 - mean_squared_error: 0.0021     \n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 9s - loss: 0.0020 - mean_squared_error: 0.0020     \n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 9s - loss: 0.0022 - mean_squared_error: 0.0022     \n",
      "Epoch 24/100\n",
      "600/600 [==============================] - 9s - loss: 0.0016 - mean_squared_error: 0.0016     \n",
      "Epoch 25/100\n",
      "600/600 [==============================] - 9s - loss: 0.0017 - mean_squared_error: 0.0017     \n",
      "Epoch 26/100\n",
      "600/600 [==============================] - 9s - loss: 0.0019 - mean_squared_error: 0.0019     \n",
      "Epoch 27/100\n",
      "600/600 [==============================] - 9s - loss: 0.0015 - mean_squared_error: 0.0015     \n",
      "Epoch 28/100\n",
      "600/600 [==============================] - 9s - loss: 0.0014 - mean_squared_error: 0.0014     \n",
      "Epoch 29/100\n",
      "600/600 [==============================] - 9s - loss: 0.0014 - mean_squared_error: 0.0014     \n",
      "Epoch 30/100\n",
      "600/600 [==============================] - 10s - loss: 0.0013 - mean_squared_error: 0.0013    \n",
      "Epoch 31/100\n",
      "600/600 [==============================] - 9s - loss: 0.0011 - mean_squared_error: 0.0011     \n",
      "Epoch 32/100\n",
      "600/600 [==============================] - 9s - loss: 0.0013 - mean_squared_error: 0.0013     \n",
      "Epoch 33/100\n",
      "600/600 [==============================] - 9s - loss: 0.0015 - mean_squared_error: 0.0015     \n",
      "Epoch 34/100\n",
      "600/600 [==============================] - 9s - loss: 0.0013 - mean_squared_error: 0.0013     \n",
      "Epoch 35/100\n",
      "600/600 [==============================] - 9s - loss: 7.7716e-04 - mean_squared_error: 7.7716e-04     \n",
      "Epoch 36/100\n",
      "600/600 [==============================] - 9s - loss: 0.0012 - mean_squared_error: 0.0012     \n",
      "Epoch 37/100\n",
      "600/600 [==============================] - 9s - loss: 9.4539e-04 - mean_squared_error: 9.4539e-04     \n",
      "Epoch 38/100\n",
      "600/600 [==============================] - 9s - loss: 0.0010 - mean_squared_error: 0.0010     \n",
      "Epoch 39/100\n",
      "600/600 [==============================] - 9s - loss: 0.0010 - mean_squared_error: 0.0010           \n",
      "Epoch 40/100\n",
      "600/600 [==============================] - 9s - loss: 7.6215e-04 - mean_squared_error: 7.6215e-04     \n",
      "Epoch 41/100\n",
      "600/600 [==============================] - 9s - loss: 0.0010 - mean_squared_error: 0.0010           \n",
      "Epoch 42/100\n",
      "600/600 [==============================] - 9s - loss: 0.0010 - mean_squared_error: 0.0010             \n",
      "Epoch 43/100\n",
      "600/600 [==============================] - 9s - loss: 8.6433e-04 - mean_squared_error: 8.6433e-04     \n",
      "Epoch 44/100\n",
      "600/600 [==============================] - 9s - loss: 7.6427e-04 - mean_squared_error: 7.6427e-04     \n",
      "Epoch 45/100\n",
      "600/600 [==============================] - 9s - loss: 5.0547e-04 - mean_squared_error: 5.0547e-04     \n",
      "Epoch 46/100\n",
      "600/600 [==============================] - 9s - loss: 7.3554e-04 - mean_squared_error: 7.3554e-04     \n",
      "Epoch 47/100\n",
      "600/600 [==============================] - 9s - loss: 7.4176e-04 - mean_squared_error: 7.4176e-04     \n",
      "Epoch 48/100\n",
      "600/600 [==============================] - 10s - loss: 0.0011 - mean_squared_error: 0.0011      \n",
      "Epoch 49/100\n",
      "600/600 [==============================] - 10s - loss: 6.6426e-04 - mean_squared_error: 6.6426e-04    \n",
      "Epoch 50/100\n",
      "600/600 [==============================] - 9s - loss: 6.1878e-04 - mean_squared_error: 6.1878e-04     \n",
      "Epoch 51/100\n",
      "600/600 [==============================] - 9s - loss: 7.3734e-04 - mean_squared_error: 7.3734e-04     \n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='mean_squared_error', patience=5)\n",
    "callbacks = [early_stop]\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=32, input_len=input_count)\n",
    "#history = estimator.fit(X_train_count.toarray(), y_train)\n",
    "history = estimator.fit(X_train_tfidf.toarray(), y_train, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='mean_squared_error', patience=5)\n",
    "callbacks = [early_stop]\n",
    "\n",
    "learning_rates = [0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001]\n",
    "models = []\n",
    "histories = []\n",
    "for lr in learning_rates:\n",
    "    # evaluate model with standardized dataset\n",
    "    estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=32, input_len=input_count, learning_rate=lr)\n",
    "    #history = estimator.fit(X_train_count.toarray(), y_train)\n",
    "    history = estimator.fit(X_train_tfidf.toarray(), y_train, callbacks=callbacks)\n",
    "    models.append(estimator)\n",
    "    histories.append(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sample=pd.read_csv('joy_test.csv',header=None, names=['index', 'tweet','emotion','score'])\n",
    "test_sample['tweet']=test_sample['tweet'].apply(lambda x : prep.preprocess_arabic_text(x))\n",
    "\n",
    "X_test = test_sample['tweet'].tolist()\n",
    "y_test = test_sample['score'].tolist()\n",
    "X_test_count = count_vect.transform(X_test)\n",
    "X_test_tfidf = tfidf.transform(X_test_count.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: R2 : 0.992522, RMSE : 0.000254\n",
      "Test: R2 : -0.039579, RMSE : 0.033070\n"
     ]
    }
   ],
   "source": [
    "y_pred = estimator.predict(X_test_tfidf.toarray())\n",
    "y_pred_train = estimator.predict(X_train_tfidf.toarray())\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "rmse = mean_squared_error(y_test, y_pred)\n",
    "rmse_train = mean_squared_error(y_train, y_pred_train)\n",
    "print(\"Train: R2 : {0:f}, RMSE : {1:f}\".format( r2_train, rmse_train ) )\n",
    "print( \"Test: R2 : {0:f}, RMSE : {1:f}\".format( r2, rmse ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8440000000000001  >>  0.64284  >>  [array(['ارب', 'اله', 'ايم', 'ايمي', 'جعل', 'جمل', 'حب', 'خير', 'رفيق',\n",
      "       'زفاف', 'سعد', 'عروس', 'فرح', 'لك', 'موعد', 'يا', 'يتم', '💍'], \n",
      "      dtype='<U22')]\n",
      "0.563  >>  0.571871  >>  [array(['ساعت', 'سعاد', 'فيهم'], \n",
      "      dtype='<U22')]\n",
      "0.43799999999999994  >>  0.521823  >>  [array(['اول', 'حقيق', 'خير', 'دعم', 'رجال', 'زعام', 'شكر', 'عرس', 'غير',\n",
      "       'قدم', 'قلب', 'كبير', 'لجميع', 'هلال'], \n",
      "      dtype='<U22')]\n",
      "0.25  >>  0.588215  >>  [array(['بس', 'تام', 'حب', 'حزن', 'حين', 'حيوي', 'شعور', 'طاق', 'ومو'], \n",
      "      dtype='<U22')]\n",
      "0.359  >>  0.578135  >>  [array(['بالي', 'حلم', 'رب', 'سما', 'مو', 'يا'], \n",
      "      dtype='<U22')]\n",
      "0.703  >>  0.699126  >>  [array(['انو', 'بتحب', 'جو', 'سعاد', 'شخص', 'عالم', 'عيد', 'فرح'], \n",
      "      dtype='<U22')]\n",
      "0.75  >>  0.667876  >>  [array(['بتهج', 'سخر', 'شتغل', 'طبيعي', 'علي', 'كفي', 'معاي', 'مو'], \n",
      "      dtype='<U22')]\n",
      "0.452  >>  0.314568  >>  [array(['اثر', 'تشويق'], \n",
      "      dtype='<U22')]\n",
      "0.172  >>  0.479073  >>  [array(['احتفال', 'اله', 'دني', 'سامح', 'صوت', 'طلع', 'فرص', 'لكم'], \n",
      "      dtype='<U22')]\n",
      "0.609  >>  0.60821  >>  [array(['اجر', 'اله', 'اهل', 'باذ', 'رح', 'فرحي', 'قبل', 'قدري', 'كما'], \n",
      "      dtype='<U22')]\n",
      "0.547  >>  0.643778  >>  [array(['جبت', 'فرح', 'فشخ'], \n",
      "      dtype='<U22')]\n",
      "0.297  >>  0.514646  >>  [array(['بس', 'خجل', 'طريق', 'مرحب', 'مكن'], \n",
      "      dtype='<U22')]\n",
      "0.613  >>  0.622723  >>  [array(['ابتسام', 'اثر', 'جميل', 'حول', 'دع', 'دين', 'سعاد', 'صحه', 'صدق',\n",
      "       'فهي', 'قلب', 'لك', 'لمن', 'ملامح', 'موسيق', '🍃'], \n",
      "      dtype='<U22')]\n",
      "0.625  >>  0.602636  >>  [array(['5050', 'اعل', 'بهج', 'ده', 'عبقري'], \n",
      "      dtype='<U22')]\n",
      "0.43799999999999994  >>  0.618942  >>  [array(['اخر', 'حفظ', 'ربي', 'رزق', 'رضا', 'شمس', 'عتم', 'عمر', 'فرح',\n",
      "       'فسي', 'قلب', 'لهم', 'ليل'], \n",
      "      dtype='<U22')]\n",
      "0.547  >>  0.501676  >>  [array(['اصو', 'رسوم', 'ضحك', 'متحرك'], \n",
      "      dtype='<U22')]\n",
      "0.344  >>  0.507934  >>  [array(['اهل', 'بلا', 'جامع', 'حد', 'ستمتع', 'سخر', 'شبع'], \n",
      "      dtype='<U22')]\n",
      "0.828  >>  0.721661  >>  [array(['الي', 'بتحلم', 'بتنام', 'بيه', 'حب', 'خلي', 'دا', 'رضي', 'سعاد',\n",
      "       'صحي', 'عاد', 'فرح', 'هتلاقي'], \n",
      "      dtype='<U22')]\n",
      "0.484  >>  0.609399  >>  [array(['جمل', 'حيا', 'سحاب', 'ضغط', 'عشا', '😅'], \n",
      "      dtype='<U22')]\n",
      "0.484  >>  0.640499  >>  [array(['ارب', 'حب', 'سعاد', 'سعد', 'قلب', 'قلبي', 'مساء', 'يا', '⚘'], \n",
      "      dtype='<U22')]\n",
      "0.823  >>  0.652203  >>  [array(['دي', 'سخر', 'شوي', 'طلع', 'عند', 'فرح', 'كبر', 'كما', 'لسه'], \n",
      "      dtype='<U22')]\n",
      "0.391  >>  0.475083  >>  [array(['سعد', 'شكر', 'شيء', 'هدا'], \n",
      "      dtype='<U22')]\n"
     ]
    }
   ],
   "source": [
    "for i in range(22,44):\n",
    "    print(y_test[-i], ' >> ', y_pred[-i], ' >> ', count_vect.inverse_transform(X_test_count[-i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
