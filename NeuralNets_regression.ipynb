{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from resources.arabic_preprocessing import Arabic_preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-ar-joy-train-1</td>\n",
       "      <td>منسم يقول لصيدلي\\n\\nعندك شريط بنادول؟\\nقاله:نع...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-ar-joy-train-2</td>\n",
       "      <td>· بتكبرين ياللي من رضا الأيام جيتي يوم ميلادك ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-ar-joy-train-3</td>\n",
       "      <td>#هدوء #عمرو دياب #مزاج ❤️❤️❤️❤️❤️</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-ar-joy-train-4</td>\n",
       "      <td>سبحان الله #رضى الوالدين له سحر عجيب</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-ar-joy-train-5</td>\n",
       "      <td>الله يرضى عليك. \\nبدل متقولي: (عيد سعيد)، قولي...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 index                                              tweet  \\\n",
       "0  2018-ar-joy-train-1  منسم يقول لصيدلي\\n\\nعندك شريط بنادول؟\\nقاله:نع...   \n",
       "1  2018-ar-joy-train-2  · بتكبرين ياللي من رضا الأيام جيتي يوم ميلادك ...   \n",
       "2  2018-ar-joy-train-3                  #هدوء #عمرو دياب #مزاج ❤️❤️❤️❤️❤️   \n",
       "3  2018-ar-joy-train-4               سبحان الله #رضى الوالدين له سحر عجيب   \n",
       "4  2018-ar-joy-train-5  الله يرضى عليك. \\nبدل متقولي: (عيد سعيد)، قولي...   \n",
       "\n",
       "  emotion  score  \n",
       "0     joy  0.422  \n",
       "1     joy  0.565  \n",
       "2     joy  0.438  \n",
       "3     joy  0.375  \n",
       "4     joy  0.438  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample=pd.read_csv('data/joy_train.csv',header=None, names=['index', 'tweet','emotion','score'])\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-ar-joy-train-1</td>\n",
       "      <td>منسم قول لصيدلي عند شريط بنادول قالهنعم قال شغ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-ar-joy-train-2</td>\n",
       "      <td>· بتكبر الي رضا ايم جيتي ميلاد فرح عمر مو اريخ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-ar-joy-train-3</td>\n",
       "      <td>هدوء عمرو دياب مزاج حب ️ حب ️ حب ️ حب ️ حب ️</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-ar-joy-train-4</td>\n",
       "      <td>اله رضي والد سحر عجيب</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-ar-joy-train-5</td>\n",
       "      <td>اله رضي بدل متقولي عيد سعيد قولي قدام 6 ليالي ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 index                                              tweet  \\\n",
       "0  2018-ar-joy-train-1  منسم قول لصيدلي عند شريط بنادول قالهنعم قال شغ...   \n",
       "1  2018-ar-joy-train-2  · بتكبر الي رضا ايم جيتي ميلاد فرح عمر مو اريخ...   \n",
       "2  2018-ar-joy-train-3       هدوء عمرو دياب مزاج حب ️ حب ️ حب ️ حب ️ حب ️   \n",
       "3  2018-ar-joy-train-4                              اله رضي والد سحر عجيب   \n",
       "4  2018-ar-joy-train-5  اله رضي بدل متقولي عيد سعيد قولي قدام 6 ليالي ...   \n",
       "\n",
       "  emotion  score  \n",
       "0     joy  0.422  \n",
       "1     joy  0.565  \n",
       "2     joy  0.438  \n",
       "3     joy  0.375  \n",
       "4     joy  0.438  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep = Arabic_preprocessing()\n",
    "sample['tweet']=sample['tweet'].apply(lambda x : prep.preprocess_arabic_text(x))\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3019\n"
     ]
    }
   ],
   "source": [
    "X = sample['tweet'].tolist()\n",
    "y_train = sample['score'].tolist()\n",
    "count_vect = CountVectorizer(lowercase=False, token_pattern=r'\\S+')\n",
    "count_vect = count_vect.fit(X)  # bag of words\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf = tfidf.fit(count_vect.transform(X))\n",
    "X_train_count = count_vect.transform(X)\n",
    "X_train_tfidf = tfidf.transform(X_train_count.toarray())\n",
    "input_count = X_train_count.shape[1]\n",
    "print(input_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the model : Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(input_len, learning_rate=0.001):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4000, input_dim=input_len, activation='relu'))\n",
    "    model.add(Dense(2000, activation='relu'))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(1000, activation='relu'))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    # Compile model\n",
    "    rmsprop = RMSprop(lr=learning_rate)\n",
    "    model.compile(loss='mse', optimizer=rmsprop, metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "**run either 'Without GridSearch' or 'Manual Grid Search', not both!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "224/600 [==========>...................] - ETA: 7s - loss: 7.9196 - mean_squared_error: 7.9196"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a7d13d1062c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKerasRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbaseline_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#history = estimator.fit(X_train_count.toarray(), y_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\ML\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\ML\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 867\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\ML\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1598\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\ML\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\ML\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2273\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2274\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='mean_squared_error', patience=5)\n",
    "callbacks = [early_stop]\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=32, input_len=input_count)\n",
    "#history = estimator.fit(X_train_count.toarray(), y_train)\n",
    "history = estimator.fit(X_train_tfidf.toarray(), y_train, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='mean_squared_error', patience=5)\n",
    "callbacks = [early_stop]\n",
    "\n",
    "learning_rates = [0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001]\n",
    "models = []\n",
    "histories = []\n",
    "for lr in learning_rates:\n",
    "    # evaluate model with standardized dataset\n",
    "    estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=32, input_len=input_count, learning_rate=lr)\n",
    "    #history = estimator.fit(X_train_count.toarray(), y_train)\n",
    "    history = estimator.fit(X_train_tfidf.toarray(), y_train, callbacks=callbacks)\n",
    "    models.append(estimator)\n",
    "    histories.append(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sample=pd.read_csv('joy_test.csv',header=None, names=['index', 'tweet','emotion','score'])\n",
    "test_sample['tweet']=test_sample['tweet'].apply(lambda x : prep.preprocess_arabic_text(x))\n",
    "\n",
    "X_test = test_sample['tweet'].tolist()\n",
    "y_test = test_sample['score'].tolist()\n",
    "X_test_count = count_vect.transform(X_test)\n",
    "X_test_tfidf = tfidf.transform(X_test_count.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: R2 : 0.992522, RMSE : 0.000254\n",
      "Test: R2 : -0.039579, RMSE : 0.033070\n"
     ]
    }
   ],
   "source": [
    "y_pred = estimator.predict(X_test_tfidf.toarray())\n",
    "y_pred_train = estimator.predict(X_train_tfidf.toarray())\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "rmse = mean_squared_error(y_test, y_pred)\n",
    "rmse_train = mean_squared_error(y_train, y_pred_train)\n",
    "print(\"Train: R2 : {0:f}, RMSE : {1:f}\".format( r2_train, rmse_train ) )\n",
    "print( \"Test: R2 : {0:f}, RMSE : {1:f}\".format( r2, rmse ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8440000000000001  >>  0.64284  >>  [array(['ارب', 'اله', 'ايم', 'ايمي', 'جعل', 'جمل', 'حب', 'خير', 'رفيق',\n",
      "       'زفاف', 'سعد', 'عروس', 'فرح', 'لك', 'موعد', 'يا', 'يتم', '💍'], \n",
      "      dtype='<U22')]\n",
      "0.563  >>  0.571871  >>  [array(['ساعت', 'سعاد', 'فيهم'], \n",
      "      dtype='<U22')]\n",
      "0.43799999999999994  >>  0.521823  >>  [array(['اول', 'حقيق', 'خير', 'دعم', 'رجال', 'زعام', 'شكر', 'عرس', 'غير',\n",
      "       'قدم', 'قلب', 'كبير', 'لجميع', 'هلال'], \n",
      "      dtype='<U22')]\n",
      "0.25  >>  0.588215  >>  [array(['بس', 'تام', 'حب', 'حزن', 'حين', 'حيوي', 'شعور', 'طاق', 'ومو'], \n",
      "      dtype='<U22')]\n",
      "0.359  >>  0.578135  >>  [array(['بالي', 'حلم', 'رب', 'سما', 'مو', 'يا'], \n",
      "      dtype='<U22')]\n",
      "0.703  >>  0.699126  >>  [array(['انو', 'بتحب', 'جو', 'سعاد', 'شخص', 'عالم', 'عيد', 'فرح'], \n",
      "      dtype='<U22')]\n",
      "0.75  >>  0.667876  >>  [array(['بتهج', 'سخر', 'شتغل', 'طبيعي', 'علي', 'كفي', 'معاي', 'مو'], \n",
      "      dtype='<U22')]\n",
      "0.452  >>  0.314568  >>  [array(['اثر', 'تشويق'], \n",
      "      dtype='<U22')]\n",
      "0.172  >>  0.479073  >>  [array(['احتفال', 'اله', 'دني', 'سامح', 'صوت', 'طلع', 'فرص', 'لكم'], \n",
      "      dtype='<U22')]\n",
      "0.609  >>  0.60821  >>  [array(['اجر', 'اله', 'اهل', 'باذ', 'رح', 'فرحي', 'قبل', 'قدري', 'كما'], \n",
      "      dtype='<U22')]\n",
      "0.547  >>  0.643778  >>  [array(['جبت', 'فرح', 'فشخ'], \n",
      "      dtype='<U22')]\n",
      "0.297  >>  0.514646  >>  [array(['بس', 'خجل', 'طريق', 'مرحب', 'مكن'], \n",
      "      dtype='<U22')]\n",
      "0.613  >>  0.622723  >>  [array(['ابتسام', 'اثر', 'جميل', 'حول', 'دع', 'دين', 'سعاد', 'صحه', 'صدق',\n",
      "       'فهي', 'قلب', 'لك', 'لمن', 'ملامح', 'موسيق', '🍃'], \n",
      "      dtype='<U22')]\n",
      "0.625  >>  0.602636  >>  [array(['5050', 'اعل', 'بهج', 'ده', 'عبقري'], \n",
      "      dtype='<U22')]\n",
      "0.43799999999999994  >>  0.618942  >>  [array(['اخر', 'حفظ', 'ربي', 'رزق', 'رضا', 'شمس', 'عتم', 'عمر', 'فرح',\n",
      "       'فسي', 'قلب', 'لهم', 'ليل'], \n",
      "      dtype='<U22')]\n",
      "0.547  >>  0.501676  >>  [array(['اصو', 'رسوم', 'ضحك', 'متحرك'], \n",
      "      dtype='<U22')]\n",
      "0.344  >>  0.507934  >>  [array(['اهل', 'بلا', 'جامع', 'حد', 'ستمتع', 'سخر', 'شبع'], \n",
      "      dtype='<U22')]\n",
      "0.828  >>  0.721661  >>  [array(['الي', 'بتحلم', 'بتنام', 'بيه', 'حب', 'خلي', 'دا', 'رضي', 'سعاد',\n",
      "       'صحي', 'عاد', 'فرح', 'هتلاقي'], \n",
      "      dtype='<U22')]\n",
      "0.484  >>  0.609399  >>  [array(['جمل', 'حيا', 'سحاب', 'ضغط', 'عشا', '😅'], \n",
      "      dtype='<U22')]\n",
      "0.484  >>  0.640499  >>  [array(['ارب', 'حب', 'سعاد', 'سعد', 'قلب', 'قلبي', 'مساء', 'يا', '⚘'], \n",
      "      dtype='<U22')]\n",
      "0.823  >>  0.652203  >>  [array(['دي', 'سخر', 'شوي', 'طلع', 'عند', 'فرح', 'كبر', 'كما', 'لسه'], \n",
      "      dtype='<U22')]\n",
      "0.391  >>  0.475083  >>  [array(['سعد', 'شكر', 'شيء', 'هدا'], \n",
      "      dtype='<U22')]\n"
     ]
    }
   ],
   "source": [
    "for i in range(22,44):\n",
    "    print(y_test[-i], ' >> ', y_pred[-i], ' >> ', count_vect.inverse_transform(X_test_count[-i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
