{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Arabic Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this module aims to preprocess arabic text (tweets specifically) in order to be efficiently used for Natural Language Processing & Machine Learning\n",
    "### Preprocessing includes:\n",
    "- tokenization\n",
    "- remove mentions\n",
    "- normalize_hashtag: remove hashtag symbol (#) and replace underscores in hashtags by spaces\n",
    "- remove punctuations & diacritics\n",
    "- normalize (or standarize) Hamza & Ha2\n",
    "- remove repeating characters\n",
    "- remove english characters\n",
    "- remove stop words\n",
    "- replace emojies using manually crafted emojis lexicon\n",
    "- stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk  # if nltk is being used for the first time, you may need to download some resources: nltk.download()\n",
    "from nltk.stem.arlstem import ARLSTem  # requires minimum nltk version of 3.2.5\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Arabic preprocessing class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Arabic_preprocessing:\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        #preparing arabic emojis lexicon\n",
    "        emojis_lexicon=pd.read_csv('resources/emoji_lexicon.csv',header=None, names=['emoji', 'utf-8','unicode','arabic_translation'])\n",
    "        self.emojis_lexicon_dict = dict()\n",
    "        for index, row in emojis_lexicon.iterrows():\n",
    "            self.emojis_lexicon_dict[row['emoji'].replace(' ','')] = row['arabic_translation']\n",
    "\n",
    "        #preparing arabic stop words list\n",
    "        f=open('resources/arabic_stop_words.txt', 'r', encoding=\"utf8\")  # read stop words from file\n",
    "        self.stop_words = [line.strip() for line in f.readlines()]  # construct list of stop wrods and remove the new line character\n",
    "\n",
    "        #preparing punctuations list\n",
    "        arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "        english_punctuations = string.punctuation\n",
    "        self.all_punctuations = set(arabic_punctuations + english_punctuations)\n",
    "\n",
    "        # initializing the stemmer\n",
    "        self.stemmer = ARLSTem()  # requires minimum NLTK version of 3.2.5\n",
    "        \n",
    "        self.arabic_diacritics = re.compile(\"\"\"\n",
    "                                         ّ    | # Tashdid\n",
    "                                         َ    | # Fatha\n",
    "                                         ً    | # Tanwin Fath\n",
    "                                         ُ    | # Damma\n",
    "                                         ٌ    | # Tanwin Damm\n",
    "                                         ِ    | # Kasra\n",
    "                                         ٍ    | # Tanwin Kasr\n",
    "                                         ْ    | # Sukun\n",
    "                                         ـ     # Tatwil/Kashida\n",
    "\n",
    "                                     \"\"\", re.VERBOSE)\n",
    "\n",
    "        \n",
    "    def normalize_arabic(self, text):\n",
    "        text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "        text = re.sub(\"ى\", \"ي\", text)\n",
    "        text = re.sub(\"ؤ\", \"ء\", text)\n",
    "        text = re.sub(\"ئ\", \"ء\", text)\n",
    "        text = re.sub(\"ة\", \"ه\", text)  # replace ta2 marboota by ha2\n",
    "        text = re.sub(\"گ\", \"ك\", text)\n",
    "        text = re.sub(\"\\u0640\", '', text)  # remove tatweel\n",
    "        return text\n",
    "\n",
    "\n",
    "    def remove_punctuations(self, text):\n",
    "        for p in self.all_punctuations:\n",
    "            if p in text:\n",
    "                text = text.replace(p, '')\n",
    "        return text\n",
    "\n",
    "\n",
    "    def remove_diacritics(self, text):\n",
    "        text = re.sub(self.arabic_diacritics, '', text)\n",
    "        return text\n",
    "\n",
    "\n",
    "    def remove_repeating_char(self, text):\n",
    "        return re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "\n",
    "    def remove_mention(self, text):\n",
    "        return re.sub(r'@\\S+', '', text)\n",
    "\n",
    "\n",
    "    def hashtag_match(self, match_object):\n",
    "        return match_object.group(1).replace('_', ' ')\n",
    "\n",
    "    def normalize_hashtag(self, text):\n",
    "        return re.sub(r'#(\\S+)', self.hashtag_match, text)\n",
    "\n",
    "    def emojis_match(self, match_object):\n",
    "        return ' ' + ' '.join(list(match_object.group(1))) + ' '\n",
    "\n",
    "    def separate_emojis(self, text):\n",
    "        emojis_unicode = r'([\\U0001F600-\\U0001F64F\\U00002000-\\U00003000]+)'\n",
    "        return re.sub(emojis_unicode, self.emojis_match, text)\n",
    "\n",
    "    def replace_emojis(self, text):\n",
    "        new_text = \"\"\n",
    "        for l in text:\n",
    "            new_text += self.emojis_lexicon_dict[l] if l in self.emojis_lexicon_dict.keys() else l\n",
    "        return new_text\n",
    "\n",
    "    def remove_english_characters(self, text):\n",
    "        return re.sub(r'[a-zA-Z]+', '', text)\n",
    "    \n",
    "    def clean_stop_words(self):\n",
    "        # normalize, and remove diacritics from, stop words to increase posibility of matching with normalized data\n",
    "        self.stop_words = [self.remove_diacritics(self.normalize_arabic(word)) for word in self.stop_words]\n",
    "    \n",
    "    def preprocess_arabic_text(self, text):\n",
    "        self.clean_stop_words()\n",
    "        text = text.replace('\\\\n', ' ').replace('\\n', ' ')\n",
    "        text = self.remove_mention(text)\n",
    "        text = self.normalize_hashtag(text)\n",
    "        text = self.remove_punctuations(text)\n",
    "        text = self.remove_diacritics(text)\n",
    "        text = self.normalize_arabic(text)\n",
    "        text = self.separate_emojis(text)\n",
    "        text = self.replace_emojis(text)\n",
    "        text = self.remove_repeating_char(text)\n",
    "        text = self.remove_english_characters(text)\n",
    "        words = nltk.word_tokenize(text)\n",
    "        words = [word for word in words if word not in self.stop_words]\n",
    "        words = [self.stemmer.stem(word) for word in words]\n",
    "        return ' '.join(words)  # return sentence (str), not list of words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
